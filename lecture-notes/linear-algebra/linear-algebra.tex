\documentclass[a4paper]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage[hidelinks]{hyperref}

% for flow chart
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\tikzstyle{block} = [rectangle, draw, 
    text width=7em, text centered, rounded corners, minimum height=4em]
\tikzstyle{arrow} = [thick,->,>=stealth]

% for the laplace operator
\DeclareMathOperator{\Lapl}{\mathcal{L}}

% Set up remark environment with amsthm
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\newtheorem*{definition}{Definition}
\newtheorem{example}{Example}
\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}

\title{\huge Linear Algebra}
\author{Darren Wee\\\texttt{darren.wee@u.nus.edu}}

\begin{document}
\maketitle
\tableofcontents

\section{Matrices}
\subsection{Introduction}
A system of linear algebraic equations in two variables may look like

\begin{align}
	2x + 7y & = 3 \\
	4x + 8y & = 11
\end{align}

This can be expressed as
\begin{equation}
	\begin{bmatrix}
	2	& 7 \\
	4 	& 8
	\end{bmatrix}
	\begin{bmatrix}
	x	\\
	y
	\end{bmatrix}
	= \begin{bmatrix}
	3 \\
	11
	\end{bmatrix}
\end{equation}

Each element in a matrix can be described using coordinates, e.g. $(i,j)$.

A vector is a special case of a matrix where it has only one row or one column.

A matrix with only one row or one column is a scalar.

\subsection{Matrix Operations}
\subsubsection{Addition and Subtraction}
For two matrices $X$ and $Y$ both with dimensions $m\times n$, the operation $X-Y$ or $X+Y$ implies element-by-element subtraction/addition respectively.

\subsubsection{Scalar Multiplication}
Scalar multiplication is also possible; for a scalar $k\in\mathbb{R}$,
\begin{equation}
	k
	\begin{bmatrix}
	a	& b \\
	c	& d
	\end{bmatrix}
	=
	\begin{bmatrix}
	ak	& bk \\
	ck 	& dk
	\end{bmatrix}
\end{equation}

\subsubsection{Matrix Transposition}
For each element $X$ at $(i,j)$, swap the positions of $X$ with its transposed element $Y$ at $(j, i)$.

The transposition of matrix $A$ is denoted with $A^T$.

\begin{equation}
	\begin{bmatrix}
	1	& 2 \\
	4 	& 8
	\end{bmatrix}^{T}
	=
	\begin{bmatrix}
	1	& 4 \\
	2	& 8
	\end{bmatrix}
\end{equation}

It is trivial to see that the diagonal of the matrix is unaffected as $(i,j) = (j,i)$ along the diagonal since $i=j$.

In the case of vector transposition, a row vector is transposed into a column vector.
The converse is also true.

\begin{theorem}
	A matrix $A$ is said to be \emph{symmetrical} if
	\begin{equation}
		A^T = A
	\end{equation}
\end{theorem}

\begin{theorem}
	A matrix $A$ can be said to be \emph{anti-symmetric} if
	\begin{equation}
		A^T = -A
	\end{equation}
\end{theorem}

\subsubsection{Matrix Multiplication}
For two matrices $X$ and $Y$ with dimensions $m\times n$ and $n\times t$, the matrix product $XY$ results in a $m\times t$ matrix.

\begin{equation}
	\begin{bmatrix}
	a_{11} & a_{12} \\
	a_{21} & a_{22}
	\end{bmatrix}
	\begin{bmatrix}
	b_{11} & b_{12} \\
	b_{21} & b_{22}
	\end{bmatrix}
	=
	\begin{bmatrix}
	a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\
	a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22}
	\end{bmatrix}
\end{equation}

Suppose the following:
\begin{equation}
	\begin{bmatrix}
	a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\
	a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22}
	\end{bmatrix}
	=
	\begin{bmatrix}
	c_{11} & c_{12} \\
	c_{21} & c_{22}
	\end{bmatrix}
\end{equation}

In general, the following is true for all matrices of any arbitrary size
\begin{equation}
	c_{mn} = \sum_j a_{mj}b_{jn}
\end{equation}

Note that $XY \neq YX$ usually, unless both matrices $X$ and $Y$ are filled with the same values.

In general, for matrices $A$ and $B$ of any time,
\begin{equation}
	(AB)^T = B^TA^T
\end{equation}
(don't forget to reverse the order of $B$ and $A$)

\subsubsection{Scalar and Vector Product in Matrices}
Suppose you are given two column vectors $\vec{u}$ and $\vec{v}$ in R$^3$.

The dot product of $\vec{u}$ and $\vec{v}$ can be described as a matrix product,
\begin{equation}
	\vec{u}\cdot\vec{v} = \vec{u}^T\vec{v}
\end{equation}

Similarly, the length of a vector $\vec{u}$ can be given by
\begin{equation}
	|\vec{u}| = \sqrt{\vec{u}\cdot\vec{u}} = \sqrt{\vec{u}^T\vec{u}}
\end{equation}

Any vector in $\mathbf{R}^3$ can be used to define an anti-symmetrical $3\times 3$ matrix.
\begin{equation}
	\vec{u}
	=
	\begin{bmatrix}
		u_1 \\
		u_2 \\
		u_3
	\end{bmatrix}
	\text{ defines }
	\begin{bmatrix}
		0	& -u_3	& u_2 \\
		u_3	& 0		& -u_1 \\
		-u_2& u_1 	& 0
	\end{bmatrix}
\end{equation}

\subsubsection{Matrix Calculus}
Calculus can be done on matrices as well.
It is simple element-by-element integration/differentiation.
The chain rule may be necessary depending on the variables inside the matrix and the variable of integration/differentiation.

\subsection{Identity Matrix}
The identity matrix is denoted by $I$ for any size $n\times n$.

In $I$, the diagonal elements are always 1 with all other elements 0.

For example, $I$ of size $2\times 2$ is given by
$\begin{bmatrix}
	1	& 0 \\
	0	& 1
\end{bmatrix}$

\begin{theorem}
	A matrix $B$ is said to be \emph{orthogonal} if
	\begin{equation}
	B^TB = I
	\end{equation}
\end{theorem}

An example of an orthogonal matrix is $\begin{bmatrix} \cos{\theta} & \sin{\theta} \\ \sin{\theta} & -\cos{\theta}\end{bmatrix}$

\section{Linear Transformations}
\subsection{Introduction}
A linear transformation is a rule that turns vectors into other vectors.

Formally,
\begin{theorem}
	If $T$ is a linear transformation mapping $\mathbf{R}^n$ to $\mathbf{R}^m$, and $\vec{x}$ is an $n$-dimensional column vector, then
	\begin{equation}
	T(\vec{x}) = \mathbf{A}\vec{x}
	\end{equation}
	where $\mathbf{A}$ is known as the \emph{transformation matrix} of $T$.
\end{theorem}

\begin{example}
	Suppose $T$ is a linear transformation that turns 3-dimensional vectors into 2-dimensional ones.
	The rules of $T$ are as follows:
	\begin{align}
		T(\hat{i})	& = 2\hat{i} \\
		T(\hat{j})	& = \hat{i} + \hat{j}\\
		T(\hat{k})	& = \hat{i} - \hat{j}
	\end{align}
	
	The transformation matrix of $T$ is thus given by
	$\begin{bmatrix}
		2 & 1 & 1 \\
		0 & 1 & -1
	\end{bmatrix}$

\end{example}

\subsubsection{Basic Properties}
Suppose a linear transformation $T$, a scalar $c\in\mathbb{R}$ and vectors $\vec{u}$, $\vec{v}$.

Then, the following equations are true:
\begin{align}
	T(c\vec{u}) 		& = cT(\vec{u}) \\
	T(\vec{u}+\vec{v}) 	& = T(\vec{u}) + T(\vec{v})
\end{align}

A linear transformation is thus some mapping that preserves the operations of addition and scalar multiplication.

\begin{example}
	Suppose we know the following for some linear transformation $T$:
	\begin{align}
		T(\hat{i}) & = \hat{i} + \frac{1}{4}\hat{j}\\
		T(\hat{j}) & = \frac{1}{4}\hat{i} + \hat{j}
	\end{align}

	We can calculate the linear transformation of some other vector easily.
	\begin{align}
		T(2\hat{i} + 3\hat{j}) 	& = 2T(\hat{i}) + 3T(\hat{j}) \\
								& = 2(\hat{i} + \frac{1}{4}\hat{j}) + 3(\frac{1}{4}\hat{i} + \hat{j}) \\
								& = \frac{11}{4}\hat{i} + \frac{7}{2}\hat{j}
	\end{align}

	This works because we know what the rule/mapping $T$ does to the basis of the vector space (in this case the basis is $\hat{i}$ and $\hat{j}$ since our vector is in $\mathbf{R}^2$).
\end{example}

\subsection{Identity Linear Transformation}
\begin{theorem}
	The linear identity transform $I$ is the rule
	\begin{equation}
		I\vec{u} = \vec{u}
	\end{equation}
	for all vectors $\vec{u}$.
\end{theorem}


\subsection{Applications of Simple Linear Transformations}
\subsubsection{Shear Forces}
Suppose you have a block of rubber and shear it.
There is no change in volume but the top of the block shifts a distance $\tan{\theta}$ in the direction of the shear relative to the base.

The shearing transformation $S$ can be thought of as
\begin{align}
	S(\hat{i}) 	& = \hat{i}\\
	S(\hat{j}) 	& = \hat{i}\tan{\theta} + \hat{j} \\
	S			& = \begin{bmatrix} 1 & \tan{\theta} \\ 0 & 1 \end{bmatrix}
\end{align}

\textbf{Shear does not add up linearly.}

\begin{example}
	Suppose you take a piece of rubber and shear it parallel to the $x$-axis by $\theta$ degrees, and then shear it again by $\phi$ degrees.
	\begin{equation}
		\begin{bmatrix}
			1	& \tan{\phi} \\
			0 	& 1
		\end{bmatrix}
		\begin{bmatrix}
			1	& \tan{\phi} \\
			0 	& 1
		\end{bmatrix}
		=
		\begin{bmatrix}
			1	& \tan{\phi} + \tan{\theta}\\
			0 	& 1
		\end{bmatrix}
	\end{equation}
\end{example}

The shear angles $\theta$ and $\phi$ do not add up linearly.

\subsubsection{Plane Rotations}
Suppose you want to rotate an entire plane through an angle $\theta$ counter-clockwise.

The rule for plane rotation $R$ can be defined as
\begin{align}
	R(\hat{i})	& = \hat{i}\cos{\theta} + \hat{j}\sin{\theta}\\
	R(\hat{j}) 	& = -\hat{i}\sin{\theta} + \hat{j}\cos{\theta} \\
	R(\theta) 	& = \begin{bmatrix} \cos{\theta} & -\sin{\theta} \\ \sin{\theta} & \cos{\theta} \end{bmatrix}
\end{align}

\subsection{Composite Transformations}
The order of priority of composite transformations is left to right.

It is necessary to ensure that the transformations on the right are able to take in the output of whatever transformation went before it as input.

\begin{example}
	Suppose the linear transformation $A$ eats vectors in $\mathbf{R}^3$ and spits out a vector in $\mathbf{R}^2$, while another linear transformation $B$ does the opposite\footnote{eats vectors in $\mathbf{R}^2$ and spits out a vector in $\mathbf{R}^3$}.
	The composite transform $AB$ would thus not make sense, but $BA$ would be okay.
\end{example}

\begin{theorem}
	Suppose $a_{ij}$ is the matrix of a linear transformation $A$ relative to $\hat{i}$, $\hat{j}$ and $\hat{k}$.
	Suppose $b_{ij}$ is the matrix of a linear transformation $B$ relative to $\hat{i}$, $\hat{j}$ and $\hat{k}$.
	And suppose that the composite transformation $AB$ is sensible.

	Then the transformation matrix $AB$ relative to $\hat{i}$, $\hat{j}$ (and $\hat{k}$) is simply the matrix product of $a_{ij}$ and $b_{ij}$.
\end{theorem}

\subsection{Determinants}
\begin{equation}
	\det 
	\begin{bmatrix}
		a	& b \\
		c	& d
	\end{bmatrix}
	=
	\begin{vmatrix}
		a & b \\
		c & d
	\end{vmatrix}
	= ad - bc
\end{equation}

\begin{equation}
	\begin{vmatrix}
	a_{11} & a_{12} & a_{13} \\
	a_{21} & a_{22} & a_{23} \\
	a_{31} & a_{32} & a_{33} 
	\end{vmatrix}
	=
	a_{11}
	\begin{vmatrix}
	a_{22} & a_{23} \\
	a_{32} & a_{33}
	\end{vmatrix}
	-a_{12}
	\begin{vmatrix}
	a_{21} & a_{23} \\
	a_{31} & a_{33}
	\end{vmatrix}
	+a_{13}
	\begin{vmatrix}
	a_{21} & a_{22} \\
	a_{31} & a_{32}
	\end{vmatrix}
\end{equation}

The above can be derived by going through each element in the first row, and striking out all the elements in the same row and column as that element.

\subsubsection{Basic Properties}
\begin{theorem}
	Let $S$ and $T$ be two linear transformations with determinants $\det{S}$ and $\det{T}$ respectively.
	\begin{equation}
		\det{ST} = \det{TS} = (\det{S}) \times (\det{T})
	\end{equation}

	i.e. the determinant of a product of composite linear transformations is independent of the order of those linear transformations.
\end{theorem}

\begin{theorem}
	If $M$ is a square matrix, then
	\begin{equation}
		\det{M^T} = \det{M}
	\end{equation}
\end{theorem}

\begin{theorem}
	If $c\in\mathbb{R}$ and $M$ is an $n\times n$ matrix, then
	\begin{equation}
		\det{cM} = c^n\det{M}
	\end{equation}
\end{theorem}

\begin{theorem}
	For an identity matrix $I$ of any size,
	\begin{equation}
		\det{I} = 1
	\end{equation}
\end{theorem}

\begin{theorem}
	Suppose an orthogonal matrix $M$ satisfies $MM^T = I$.
	\begin{align}
		\det{MM^T} 	& = \det{M}\det{M^T} \\
					& = (\det{M})^2 \\
					& = 1 \\
		\det{M} 	& = \pm 1
	\end{align}
	
	This is true for any orthogonal matrix $M$.
\end{theorem}

\begin{theorem}
	Let $T$ be some linear transformation. Suppose the following:
	\begin{equation}
		T\vec{w} = \vec{0}
	\end{equation}

	$\vec{w}$ is \textbf{not} the zero vector.
\end{theorem}

\subsection{Inverses}
Any linear transformation must map a vector to a single resultant vector.
Multiple input vectors can map to the same resultant vector.

\subsubsection{Singular Linear Transformation}
Suppose a linear transformation $T$ that is singular.

$T$ thus meets the following conditions:
\begin{enumerate}
	\item maps 2 different vectors to 1 vector
	\item destroys all of the vectors in $\geq$ 1 direction
	\item loses all information associated with those directions
	\item $\det{T} = 0$
\end{enumerate}

Singular transformations \textbf{do not} have inverses.

\begin{example}
	Suppose a linear transformation $T = \begin{bmatrix} 1 & 1 \\ 1 & 1\end{bmatrix}$.

	\begin{equation}
		\begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}
		\begin{bmatrix} 3 \\ 4\end{bmatrix}
		=
		\begin{bmatrix} 7 \\ 7\end{bmatrix}
	\end{equation}
	\begin{equation}
		\begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}
		\begin{bmatrix} 4 \\ 3\end{bmatrix}
		=
		\begin{bmatrix} 7 \\ 7\end{bmatrix}
	\end{equation}

	The determinant of the linear transformation is exactly 0, i.e. $\det{T} = 0$.
\end{example}

\subsubsection{Non-Singular Linear Transformation}
A non-singular transformation \textbf{never} maps 2 vectors to 1.

For some linear transformation $T$ and a vector $\vec{u}$, $T(\vec{u})$ maps to exactly one unique vector $\vec{v}$.

There is thus some operation $T^{-1}$ that maps $\vec{v}$ back to $\vec{u}$, i.e. $T^{-1}(\vec{v}) = \vec{u}$.
$T^{-1}$ can only exist if and only if $\det{T} \neq 0$.

\begin{example}
	Suppose a linear transformation $\begin{bmatrix} 0 & -1 \\ 1 & 0\end{bmatrix}$ that acts on vectors $\begin{bmatrix} \alpha \\ \beta\end{bmatrix}$ and $\begin{bmatrix} a \\ b \end{bmatrix}$.
	\begin{equation}
		\begin{bmatrix} 0 & -1 \\ 1 & 0\end{bmatrix}
		\begin{bmatrix} \alpha \\ \beta \end{bmatrix}
		=
		\begin{bmatrix} 0 & -1 \\ 1 & 0\end{bmatrix}
		\begin{bmatrix} a \\ b \end{bmatrix}
	\end{equation}
	
	This works out to $\begin{bmatrix} \alpha \\ \beta\end{bmatrix} = \begin{bmatrix} a \\ b\end{bmatrix}$, so the two vectors are the same.

	Ergo, this transformation never maps two different vectors to the same resultant vector --- no information is lost and $\det{T} \neq 0$.
\end{example}

\begin{theorem}
	\begin{equation}
		T^{-1}(T(\vec{u})) = \vec{u} = T(T^{-1}(\vec{u}))
	\end{equation}
\end{theorem}

\begin{theorem}
	\begin{equation}
		T^{-1}T = TT^{-1} = I
	\end{equation}
\end{theorem}

\subsubsection{Calculating the Inverse}
The inverse of any matrix $\begin{bmatrix} a & b \\ c & d\end{bmatrix}$ can be easily calculated by
\begin{equation}
	\begin{bmatrix} a & b \\ c & d\end{bmatrix}
	=
	\frac{1}{ad-bc}\begin{bmatrix} d & -b \\ -c & a\end{bmatrix}
\end{equation}

\begin{theorem}
	\begin{equation}
		(AB)^{-1} = B^{-1}A^{-1}
	\end{equation}
	Note the order of $A$ and $B$.
\end{theorem}

This notion of an inverse can be used to solve systems of linear equations.

\begin{example}
	Any system of linear equations can be written as
	\begin{equation}
		M\vec{r} = \vec{a}
	\end{equation}
	where $M$ is a matrix, $\vec{r}$ is a vector of variables and $\vec{a}$ is a vector of coefficients for the linear system.
	
	If $\det{M} \neq 0$, then there is exactly one unique solution
	\begin{equation}
		\vec{r} = M^{-1}\vec{a}
	\end{equation}

	If $\det{M} = 0$, then there is either no solution or infinitely many solutions.
\end{example}

\subsection{Eigenvectors \& Eigenvalues}
Most linear transformations usually change the direction of a vector.
However, there are some special vectors that remain invariant even after a linear transform.
These vectors are known as \emph{eigenvectors}.

\begin{theorem}
	In general, if a transformation $T$ does not change the direction of a vector $\vec{u}$, i.e. 
	\begin{equation}
		T\vec{u} = \lambda\vec{u}
	\end{equation}

	for some $\lambda\in\mathbb{R}$, then $\vec{u}$ is known as the eigenvector of $T$ and $\lambda$ the eigenvalue of $\vec{u}$.

	As a consequence of this, the null vector $\vec{0}$ is always an eigenvector (the trivial eigenvector).
\end{theorem}

\begin{example}
	Suppose a linear transform $\begin{bmatrix} 1 & 2 \\ 2 & -2\end{bmatrix}$.

	\begin{equation}
		\begin{bmatrix} 1 & 2 \\ 2 & -2\end{bmatrix}
		\begin{bmatrix} 2 \\ 1\end{bmatrix}
		=	
		\begin{bmatrix} 4 \\ 2\end{bmatrix}
		=
		2\begin{bmatrix} 2 \\ 1\end{bmatrix}
	\end{equation}

	The resulting vector from the linear transformation is parallel to its original input vector.
	Thus, we say that $\begin{bmatrix} 2 \\ 1\end{bmatrix}$ is the eigenvector of the linear transform and $2$ is its eigenvalue.
\end{example}

\subsubsection{Computing Eigenvectors and Eigenvalues}
Let $T$ denote a linear transformation, $\vec{u}$ denote an eigenvector and $\lambda$ its eigenvalue.
We assume that $\vec{u} \neq \vec{0}$ to eliminate the trivial case.
The following equation is thus taken to be true.
\begin{equation}
	T\vec{u} = \lambda\vec{u}
\end{equation}

If we let $\vec{u} = I\vec{u}$, where $I$ denotes the identity matrix, then
\begin{equation}
	(T-\lambda I)\vec{u} = \vec{0}
\end{equation}

This means that $T-\lambda I$ is a singular linear transformation as multiple vectors can be mapped to $\vec{0}$, i.e. 
\begin{equation}
	\det{(T-\lambda I)} = 0
\end{equation}

This equation can thus be solved to find $\lambda$.

\begin{example}
	Find the eigenvalues of $\begin{bmatrix} 1 & 2 \\ 2 & -2\end{bmatrix}$.

	\begin{align}
		\det{(\begin{bmatrix} 1 & 2 \\ 2 & -2\end{bmatrix} - \lambda\begin{bmatrix} 1 & 0 \\ 0 & 1\end{bmatrix})} = 0 \\
		\det{\begin{bmatrix} 1-\lambda & 2 \\ 2 & -2-\lambda\end{bmatrix}} = 0 \\
		-(-1-\lambda)(2+\lambda) - 4 = 0
	\end{align}

	From here we can see that $\lambda = 2$ or $-3$.
\end{example}

\begin{theorem}
	In general, there are $n$ eigenvalues for an $n\times n$ linear transformation.
\end{theorem}

\begin{remark}
	If $\vec{u}$ is an eigenvector of $T$, then $2\vec{u}$ is also an eigenvector \emph{but with the same eigenvalue}.
	\begin{equation}
		T(2\vec{u}) = 2T(\vec{u}) = 2\lambda\vec{u} = \lambda\times(2\vec{u})
	\end{equation}
\end{remark}

Using the calculated eigenvalues from the determinant $=0$ equation, we can find the eigenvectors by simply setting one of the dimensions of the eigenvector to some real number.

\begin{remark}
	A matrix of real numbers can have complex eigenvalues and eigenvectors.
\end{remark}

\subsection{Diagonal Form of a Linear Transformation}
Recall that the matrix of a linear transformation $T$ w.r.t. $\hat{i}, \hat{j}$ and $\hat{k}$ can be defined by a series of concatenated column vectors.

For example, let the linear transformation $T = \begin{bmatrix} a & b \\ c & d\end{bmatrix}$ w.r.t. $\hat{i}$ and $\hat{j}$, i.e.
\begin{align}
	T(\hat{i}) = a\hat{i} + c\hat{j} \\
	T(\hat{j}) = b\hat{i} + d\hat{j}
\end{align}

We can do this as $\hat{i}$ and $\hat{j}$ are known as \emph{basis vectors}.
In this case, they are the basis for $\mathbf{R}^2$.

Any vector in $\mathbf{R}^2$ can thus be expressed as a linear combination of these bases $\hat{i}$ and $\hat{j}$.
This is true for any basis that meet the conditions stated below.

\begin{theorem}
	Suppose $\vec{u} = P_{11}\hat{i} + P_{21}\hat{j}$ and $\vec{v} = P_{12}\hat{i} + P_{22}\hat{j}$.
	In order for the vectors $\vec{u},\vec{v}$ to be a basis, the following condition must be met:
	\begin{equation}
		\det{P} \neq 0
	\end{equation}

	In general, any set of $n$ vectors forms a basis in $\mathbf{R}^n$ space provided that the concatenated matrix of those vectors has a non-zero determinant.
\end{theorem}

\end{document}
